import mechanicalsoup
from bs4 import BeautifulSoup, NavigableString, Tag
import csv
import time 
import os 

### CONFIGURATION START

# Gears Data Source:
# Intersport Main searching page
url = "https://www.intersportrent.ch/en/rent-shop/products/"

#Local Data folder
data_folder = "../Data"
# locations of resorts file (input)
locations_csv = os.path.join(data_folder, 'cip_snow_reports.csv')
# gear files (output)
gears_csv = os.path.join(data_folder, 'gears_stage1.csv')

#FIX  Dates for scraping : 1 Day rental, 2 Days rental & 3 Days Rental
rent_dates = [{'first_rental_day':'2023-12-15', 'return_date':'2023-12-15'},
         {'first_rental_day':'2023-12-15', 'return_date':'2023-12-16'},
         {'first_rental_day':'2023-12-15', 'return_date':'2023-12-17'},
         ]
### CONFIGURATION END

#### Functions
def extract_gear_data(query):
   """ Extract gears offer data  from Intersport for a specific query dictionary
    Parameters:
    query (dictionary): 
                       Example of such query 
                       { "location_town" : "Arosa", 
                         "first_rental_day" : "2023-12-15",
                         "return_date" : "2023-12-16")
                        } 
    Returns:
    list: list of dicts, each element in the dict represent a gear offering for the query 
    """
   try:
      gears = [] # all gears found by the query
      # get the browser for the INTERSPORT url   
      try:
         browser = mechanicalsoup.StatefulBrowser()
         browser.open(url)
      except Exception as e:
         if type(e).__name__ == "ConnectionError":
            return gears
      # get the searching criteria form
      browser.select_form('form[id="portal-search"]' )
    
      #set QUERY parameters
      browser["q"] = query.get("location_town")
      browser["from"] = query.get("first_rental_day")
      browser["till"] = query.get("return_date")
      
      # Submit the form
      response = browser.submit_selected()
      
      # Iterate each gear result
      results = browser.page.find(class_="row gy-4 product-grid")
   
      # filter irrrelavant objects  
      filtered_results = [result for result in results if not isinstance(result, NavigableString) and not isinstance(result, Tag) ]
      
      # iterate the returned list of gears offer and extract data
      for result in filtered_results :
         try:
            # for a specific gear offer extract data
            gear = query.copy()
            #gear name
            gear['gear_name'] = result.a.get_text().strip()
         
            # gear has N free text descriptions: append free texts description relativer to gear
            # This string may be valuable for exploration more than analysys
            # as will be very sparse and unstructured
            description = ""
            li_elements = result.find(class_="product-item__list").find_all('li')
            for li in li_elements:
               if len(description) > 0:
                  description = f"{description}__{li.text}"
               else:
                  description = li.text
            gear['description'] = description
         
            #gear price for full rental period
            price = result.find(class_="fz13 me-1").text 
            gear['price'] = price

            # append to the list of gear object to be returned
            gears.append(gear.copy())
         except Exception as e:
            print(type(e).__name__)
            print(e)
   except TypeError as e:
      # no shop in the location : for debug purpose
      with open('no_shop.csv', 'a', newline='') as csvfile:
         csvfile.write(f"""{query.get("location_town")}\n""")
   except Exception as e:
      print(f"Exception type: {type(e).__name__}")

   return gears

def get_locations(locations_csv):
   """ Extract the cities from the locations csv
       thst was generated by querying myswitzerland
       This will be the list of queried city for Intersport
          Parameters:
          locations_csv (file path): 
                        
          Returns:
          list: list of cities to be queried
   """

   locations = []
   with open(locations_csv, 'r', newline='', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        location = row['location']
        locations.append(location)
   return locations
   

def build_queries():
   """ Build the queries to be executed by the scraper
       Parameters:
          None, use the Configuration section 

       Returns:
          list: list of dictionaries, each dict a query
   """
   queries = []
   # what to scrape from confuguration: list of cities
   locations = get_locations(locations_csv)
   
   # for each city and for each  defined first_rental , return_date
   for location in locations:
      for rent_date in rent_dates:
         query = { "location" : location, 
                  "location_town" : location.split('â€“')[0], 
                  "first_rental_day" : rent_date.get('first_rental_day'),
                  "return_date" : rent_date.get('return_date')
                  }
         queries.append(query)
   return queries

if __name__ == "__main__":
   # Build the Queries to be scraped for LOCATIONS and RENTAL DATES
   queries = build_queries()

   #Extracted data will be here
   gears = []

   # Run each single query
   for query in queries:
      #this query will be run
      print(query)
      #scrape query
      location_gears = extract_gear_data(query)
      #add to final result
      gears.extend(location_gears)

   # persist the data
   with open(gears_csv, 'w', newline='', encoding='utf-8') as file:
      fieldnames = gears[0].keys()
      writer = csv.DictWriter(file, fieldnames=fieldnames)
      writer.writeheader()
      writer.writerows(gears)
